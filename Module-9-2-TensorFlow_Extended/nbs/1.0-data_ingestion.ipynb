{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6c6fe6-cd3c-42a0-8b25-ba887ada63b5",
   "metadata": {},
   "source": [
    "Alexander S. Lundervold, 28.03.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fe6c2-4224-4f39-a464-e1534b16a3cd",
   "metadata": {},
   "source": [
    "# Data ingestion with TensorFlow Extended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d115ae9-f437-4694-a18c-1f70d07ffeb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "To get a pipeline up and running, we first need to ingest datasets. This notebook introduces some concepts and tools for data ingestion, in particular the `ExampleGen` component of TensorFlow Extended. We'll also connect two components together--`ExampleGen` and `StatisticsGen`, providing the first step of our ML pipeline journey. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17774433-8247-49f8-9582-46646729e2f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data set\n",
    "\n",
    "We'll use the simplified PetFinder.my data set downloaded in `0.0-prepare_data.ipynb`. The `0.0` notebook must be run before this one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43955e-244c-4a73-8bc8-6de4a97b8295",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/alu042/DAT255-2022/raw/master/Module-9-2-TensorFlow_Extended/nbs/assets/petfinder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33bd8d8-6bce-47e6-a47d-97d10857980e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037fd29-01b9-4da6-84ec-bebefcab9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a743b-4763-4bf4-bac6-bccea6ffa4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether we're running on Colab\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed77a8-89fe-4ecd-b8e1-d8c9aeb71fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !pip install -U tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86423f08-897e-4d58-aab5-b7d0d710a144",
   "metadata": {},
   "source": [
    "> If on Colab, restart the runtime after running the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b80eb-cc3d-406a-a963-b9ff9dc58c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21dd0b-5f8e-49ee-9dc1-b2337c33b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc564f00-5f79-400d-866c-0867a8cc6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf26fa1-70d8-4b32-92e3-98ce15906134",
   "metadata": {},
   "source": [
    "The data is stored locally in the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6892b-d107-4174-89b2-cfade1231473",
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('./gdrive')\n",
    "    DATA = Path('./gdrive/MyDrive/ColabData/petfinder-mini/csv')\n",
    "else:\n",
    "    NB_DIR = Path.cwd()\n",
    "    DATA = NB_DIR/'..'/'data'/'petfinder-mini'/'csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27653390-48e1-4a6a-b8cc-4c5ee3ca522f",
   "metadata": {},
   "source": [
    "## Run in an interactive context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15beca-dcec-4660-a877-79a10c5ca7b5",
   "metadata": {},
   "source": [
    "When executing components in production one would use an orchestration engine, by specifying all the components in a `Pipeline` upfront and passing them to the orchestrator. The component execution order is determined by constructing a directed acyclic graph of the artifact dependencies. \n",
    "\n",
    "During development in Jupyter Notebook it's convenient to play the role as an orchestrator ourselves, running the notebook cells as ususal. This can be achieved using `InteractiveContext`, made for iterative development in Notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0ef01-b15e-4446-a55d-ba82448fb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9504fb7-702a-4198-85d8-b1afb1a1db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = InteractiveContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734aa962-570e-4488-93b8-7071755b5491",
   "metadata": {},
   "source": [
    "> In an interactive context, the MetadataStore will use an in-memory (i.e., ephemeral) database instance based on SQLite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340c6d8-1892-4597-abe5-bff42f8eca16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ingesting the PetFinder data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f7e55-2909-4c98-b49d-a5dce11ab4c2",
   "metadata": {},
   "source": [
    "There are many ways to ingest data into our pipelines. From local or remote disks (f.ex. storage buckets like S3, GCS, etc) or directly from databases. We can ingest CSVs, TFRecords, Parquet, Avro, use BigQuery, Hadoop, Google Cloud Datastore, MongoDB, and much more (whatever is directly supported by TFX or [supported by Apache Beam](https://beam.apache.org/documentation/io/built-in/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38585598-ce62-4617-8327-6414eac7968e",
   "metadata": {},
   "source": [
    "We'll limit the below examples to ingesting from CSVs and TFRecords. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32eda5-1c79-4100-8992-5c941ff0a59a",
   "metadata": {},
   "source": [
    "## From CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05372d25-c0e0-49dc-acc1-2e1ba90cb9fc",
   "metadata": {},
   "source": [
    "There's a custom `ExampleGen` component for CSVs, based on the `FileBasedExampleGen` which is based on Apache Beam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7a89c-3857-49a9-9792-ab24d2d9ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import CsvExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a80a1-d5e5-4ca4-97f6-cdd7ba54c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?CsvExampleGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c973fc-1155-4ca6-a5e2-80c6be97bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen = CsvExampleGen(input_base=str(DATA)+'/', \n",
    "                            input_config=None, output_config=None, \n",
    "                            range_config=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a29af4-8cad-4c56-aaf9-f5706e586b26",
   "metadata": {},
   "source": [
    "Here's how `CsvExampleGen` is built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde653b8-3c74-43ed-b6b3-0428499d5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#??example_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e6171",
   "metadata": {},
   "source": [
    "### Apache Beam and TFX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dca937",
   "metadata": {},
   "source": [
    "From https://www.tensorflow.org/tfx/guide/beam:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e42ccc",
   "metadata": {},
   "source": [
    "> _Apache Beam provides a framework for running batch and streaming data processing jobs that run on a variety of execution engines. Several of the TFX libraries use Beam for running tasks, which enables a high degree of scalability across compute clusters. Beam includes support for a variety of execution engines or \"runners\", including a direct runner which runs on a single compute node and is very useful for development, testing, or small deployments. Beam provides an abstraction layer which enables TFX to run on any supported runner without code modifications. TFX uses the Beam Python API, so it is limited to the runners that are supported by the Python API._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47889b7e-5274-4e84-8a01-be481092470b",
   "metadata": {},
   "source": [
    "## `ExampleGen`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab13192-46d7-4cc0-82c6-d0916c6b52f7",
   "metadata": {},
   "source": [
    "Let's have a look at what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc1012c-e913-4110-9cc8-7236b4d60e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007706de-2cb9-4b93-9762-db665fdbb5ce",
   "metadata": {},
   "source": [
    "There are currently no output artifacts as the component hasn't been run yet. In production, an orchestrator would execute the component as needed. During interactive development we can execute it in an interactive context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5decf-e194-4779-b934-034478e3f66f",
   "metadata": {},
   "source": [
    "As we've seen, each TFX component has a **component specification**, an **executor class**, and some **inputs** and **outputs** wiring it to other components in a pipeline. We'll have a look at each of these below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e8eea-48ad-4878-bb0a-14f6723b0e18",
   "metadata": {},
   "source": [
    "### Component specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11da8c6-0826-4bad-bdb6-76968a60b642",
   "metadata": {},
   "source": [
    "The component specification is what's printed above. Here it is again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef04f3f-f4fa-4720-a3e9-49aef71a3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen.spec.to_json_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879633e2-4195-4a91-a819-aa830f9d9866",
   "metadata": {},
   "source": [
    "## Data spans and versioning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b3ff6-54e5-4ec6-9094-31e3c1952ec5",
   "metadata": {},
   "source": [
    "> In our case, all the data is stored in a single _[Span](url)_, in the directory `DATA`. In practice, data would typically be split into multiple spans, for example stored in separate directories. A span is a snapshot of data, which can for example correspond to data generated per day, week, or any other grouping that makes sense in a specific use-case. This also allows for data versioning, which is a key concept for ML pipelines. Otherwise it is difficult to track changes in a machine learning-based program (which of course depends on the exact training data used during construction). https://www.tensorflow.org/tfx/guide/examplegen#span_version_and_split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3446337a-3e50-4967-8d82-e6872eb4a6ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149f3bd-4f93-4d77-9d50-0333f7e181e4",
   "metadata": {},
   "source": [
    "When we constructed our `ExampleGen` above it set up a default split into a 2:1 ratio (train versus eval). We can customize the split, f.ex. split into a 7:2:1 ratio (70%, 20%, 10% for training, validation, testing), by defining a `split_config` in the `output_config` (which is passed to the `ExampleGen` component as a [protocol message](https://developers.google.com/protocol-buffers/docs/overview)). Note that it is of course possible to use predefined splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d869b32-2d56-4b97-b6a9-946312689c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.proto import example_gen_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bdd315-076f-47e4-958e-2fffad455688",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = example_gen_pb2.Output(\n",
    "                split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "                    example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=7),\n",
    "                    example_gen_pb2.SplitConfig.Split(name='valid', hash_buckets=2),\n",
    "                    example_gen_pb2.SplitConfig.Split(name='test', hash_buckets=1)\n",
    "                        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb689cd-fbd9-49b0-980e-7d5534489524",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen = CsvExampleGen(input_base=str(DATA)+'/', \n",
    "                            input_config=None, output_config=output, \n",
    "                            range_config=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feeb94b-02d3-44e8-b78f-60a062810873",
   "metadata": {},
   "source": [
    "Now we can see that there are splits in the component specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61983fd9-863f-4683-bca0-2824926b23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28723738-e21c-46ae-a53f-bdca591b3e3b",
   "metadata": {},
   "source": [
    "## Execute the `ExampleGen`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8829472-8f35-4329-8d74-0c713d559791",
   "metadata": {},
   "source": [
    "We run the component to obtain some artifacts. The executor converts CSV files into TensorFlow Examples (`tf.train.Examples`) using a Beam pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4bd79-9b39-44f5-ba16-ccf9befb450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e1aa3-ae41-4eb3-bcda-6060f84b77a1",
   "metadata": {},
   "source": [
    "Here are the artifacts. They are put in the underlying database and referenced in the `MetadataStore` (note the URIs below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083677b-8899-48ba-b379-8a7767048aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artifact in example_gen.outputs['examples'].get():\n",
    "    print(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651518b-a37c-481c-bb6a-16d6ec3c7e34",
   "metadata": {},
   "source": [
    "## Taking a look at some training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2c821-c107-4b20-a64b-299f2d2c8f82",
   "metadata": {},
   "source": [
    "We can look at the first couple of training examples stored as artifacts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06bc76-5fd7-4b2e-acc0-390470cc67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = Path(example_gen.outputs['examples'].get()[0].uri)/\"Split-train\"\n",
    "train_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116a66a-a81f-49b6-b698-3e5a25b4fe44",
   "metadata": {},
   "source": [
    "We see that they are TFRecord files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ee53b-aaaf-443f-8ccb-abc4c9a2e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_uri.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739154c-c48b-4626-8973-72c2134ae005",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_fn = list(train_uri.iterdir())[0]\n",
    "tfrecord_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec5b28-a6a5-4326-9ed7-07fd51b4ea42",
   "metadata": {},
   "source": [
    "From our previous explorations of TensorFlow, we know that we can load them as `TFRecordDataset`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d965112-647e-4c95-b551-329e176f6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(tfrecord_fn, compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dfead1-11f3-494d-b9d8-1474723e3923",
   "metadata": {},
   "source": [
    "Here are the first two records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9339703-b90e-4cea-af07-772726943b8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for tfrecord in train_dataset.take(2):\n",
    "    serialized_example = tfrecord.numpy()\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(serialized_example)\n",
    "    print(example)\n",
    "    print(\"#\"*40)\n",
    "    print(\"#\"*40)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5732ee-0294-4f7f-8acd-28833a92fbc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A closer look at the executor specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e99cb8-f84b-4b15-9e80-d0dce2f76d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen.executor_spec.to_json_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdbfae9-74ac-4ebc-be3a-85be28fb0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#??tfx.components.example_gen.csv_example_gen.executor.Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd105361-8645-4e11-834a-d178cf04002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#??tfx.components.example_gen.base_example_gen_executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbec55c-3601-4398-a6b6-c45c6e9b9fd8",
   "metadata": {},
   "source": [
    "# Connecting to a `StatisticsGen`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e47cb-b52d-428e-8eea-7e6be50a6557",
   "metadata": {},
   "source": [
    "To illustrate how to connect TFX components, let's compute some statistics on the data using `StatisticsGen`. \n",
    "\n",
    "> In the next notebook we'll continue using this to look into data validation strategies. Here the main point is just to show how components can be assembled into pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90d171-dccd-4818-b980-9a9400f30b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import StatisticsGen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979a7b6-cce9-4b8a-a403-38a3610b1b10",
   "metadata": {},
   "source": [
    "We define the component inputs by connecting it to our `ExampleGen`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e705d-5df9-4f7d-944c-d92d02eb94f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics_gen = StatisticsGen(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=None,\n",
    "        stats_options=None,\n",
    "        exclude_splits=None\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bd47c-cf54-41a9-8689-85cb6847fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ee1dd-0618-4866-b36a-d485a0017139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?statistics_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f84238-7bc2-4a4d-9af5-22a25ceebeea",
   "metadata": {},
   "source": [
    "Let's run the `StatisticsGen` component to generate some artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c2e80-225e-4d21-a07a-120f7a471181",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8704e26-a067-41ca-b040-2bce67d0c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artifact in statistics_gen.outputs['statistics'].get():\n",
    "    print(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6276cc2-29b6-46d9-94ab-a07c327439ca",
   "metadata": {},
   "source": [
    "We can show the computed statistics in our interactive context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512e1c7-a6ae-411b-8b44-6ef9cc428b50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f0d49-5103-474c-af6e-39286eb3e8ee",
   "metadata": {},
   "source": [
    "We now have two components connected in a pipeline: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c132f-aab4-42c4-8ab9-a5ea3462bdd1",
   "metadata": {},
   "source": [
    "<img width=60% src=\"https://github.com/alu042/DAT255-2022/raw/master/Module-9-2-TensorFlow_Extended/nbs/assets/pipeline_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf83e18-f10c-4ff5-be96-ef38fcec4ce0",
   "metadata": {},
   "source": [
    "# What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88295e0a-8c77-4044-a838-1a0e3dd2ed42",
   "metadata": {},
   "source": [
    "We'll explore how we can use the generated statistics to generate a **data schema** and perform various **data validation** steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfx]",
   "language": "python",
   "name": "conda-env-tfx-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
